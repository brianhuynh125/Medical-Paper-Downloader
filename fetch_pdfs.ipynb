{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and command line arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=argparse.ArgumentParser()\n",
    "parser._optionals.title = \"Flag Arguments\"\n",
    "parser.add_argument('-pmf',help=\"File with pmids to fetch inside, one pmid per line. Optionally, the file can be a tsv with a second column of names to save each pmid's article with (without '.pdf' at the end). Must include -pmids or -pmf\", default='open_access_pmids.txt')\n",
    "parser.add_argument('-out',help=\"Output directory for fetched articles.  Default: fetched_pdfs\", default=\"fetched_pdfs\")\n",
    "parser.add_argument('-errors',help=\"Output file path for pmids which failed to fetch.  Default: unfetched_pmids.tsv\", default=\"unfetched_pmids.tsv\")\n",
    "parser.add_argument('-maxRetries',help=\"Change max number of retries per article on an error 104.  Default: 3\", default=3,type=int)\n",
    "args = vars(parser.parse_args([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args['out']):\n",
    "    print( \"Output directory of {0} did not exist.  Created the directory.\".format(args['out']))\n",
    "    os.mkdir(args['out'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug space.  Clear before commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMainUrl(url):\n",
    "    return \"/\".join(url.split(\"/\")[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(pdf_url, out_dir, filename):\n",
    "    try:\n",
    "        r = requests.get(pdf_url, stream=True, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        with open(os.path.join(out_dir, filename), \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {filename}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landing_url(paper):\n",
    "    if paper[\"doi\"] and paper[\"doi\"] != \"none\":\n",
    "        return f\"https://doi.org/{paper['doi']}\"\n",
    "    elif paper[\"pmid\"] and paper[\"pmid\"] != \"none\":\n",
    "        return f\"https://pubmed.ncbi.nlm.nih.gov/{paper['pmid']}/\"\n",
    "    elif paper[\"pmcid\"] and paper[\"pmcid\"] != \"none\":\n",
    "        return f\"https://www.ncbi.nlm.nih.gov/pmc/articles/{paper['pmcid']}/\"\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_finder_wrapper(paper, html_finder_func, headers=None):\n",
    "    url = get_landing_url(paper)\n",
    "    if not url:\n",
    "        return None\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        return html_finder_func(r, soup, headers)\n",
    "    except Exception as e:\n",
    "        print(f\"{html_finder_func.__name__} error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finder_europepmc(paper):\n",
    "    if not paper[\"pmid\"]:\n",
    "        return None\n",
    "    url = f\"https://www.ebi.ac.uk/europepmc/webservices/rest/search?query=EXT_ID:{paper['pmid']}&resultType=core&format=json\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        for result in data.get(\"resultList\", {}).get(\"result\", []):\n",
    "            if \"fullTextUrlList\" in result:\n",
    "                for link in result[\"fullTextUrlList\"][\"fullTextUrl\"]:\n",
    "                    if link.get(\"documentStyle\") == \"pdf\":\n",
    "                        return link[\"url\"]\n",
    "    except Exception as e:\n",
    "        print(f\"EuropePMC error: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finder_unpaywall(paper):\n",
    "    if not paper[\"doi\"]:\n",
    "        return None\n",
    "    url = f\"https://api.unpaywall.org/v2/{paper['doi']}?email=YOUR_EMAIL@example.com\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        loc = data.get(\"best_oa_location\")\n",
    "        if loc and loc.get(\"url_for_pdf\"):\n",
    "            return loc[\"url_for_pdf\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Unpaywall error: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finder_core(paper):\n",
    "    if not paper[\"doi\"]:\n",
    "        return None\n",
    "    api_key = \"YOUR_CORE_API_KEY\"\n",
    "    url = f\"https://core.ac.uk:443/api-v2/articles/get/{paper['doi']}?metadata=true&fulltext=true&citations=false&similar=false&duplicate=false&urls=true&apiKey={api_key}\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        if data.get(\"data\") and data[\"data\"].get(\"downloadUrl\"):\n",
    "            return data[\"data\"][\"downloadUrl\"]\n",
    "    except Exception as e:\n",
    "        print(f\"CORE error: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finder_openaire(paper):\n",
    "    if not paper[\"doi\"]:\n",
    "        return None\n",
    "    url = f\"https://api.openaire.eu/search/publications?doi={paper['doi']}&format=json\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        # You may need to parse for PDF links here\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAIRE error: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finder_arxiv(paper):\n",
    "    if paper[\"doi\"] and paper[\"doi\"].startswith(\"10.48550/arXiv.\"):\n",
    "        arxiv_id = paper[\"doi\"].split(\"arXiv.\")[1]\n",
    "        return f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finder_springer(paper):\n",
    "    if not paper[\"doi\"]:\n",
    "        return None\n",
    "    return f\"https://link.springer.com/content/pdf/{paper['doi']}.pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acsPublications(req, soup, headers):\n",
    "    possibleLinks = [x for x in soup.find_all('a') if isinstance(x.get('title'), str) and ('high-res pdf' in x.get('title').lower() or 'low-res pdf' in x.get('title').lower())]\n",
    "    if possibleLinks:\n",
    "        print(\"** fetching reprint using the 'acsPublications' finder...\")\n",
    "        pdfUrl = getMainUrl(req.url) + possibleLinks[0].get('href')\n",
    "        return pdfUrl\n",
    "    return None\n",
    "\n",
    "def finder_acsPublications(paper):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    return html_finder_wrapper(paper, acsPublications, headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genericCitationLabelled(req, soup, headers):\n",
    "    possibleLinks = soup.find_all('meta', attrs={'name': 'citation_pdf_url'})\n",
    "    if possibleLinks:\n",
    "        print(\"** fetching reprint using the 'generic citation labelled' finder...\")\n",
    "        return possibleLinks[0].get('content')\n",
    "    return None\n",
    "def finder_genericCitationLabelled(paper):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    return html_finder_wrapper(paper, genericCitationLabelled, headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_paper_pdf(paper, out_dir, finders, error_file, max_retries=3):\n",
    "    filename = paper[\"doi\"] or paper[\"pmid\"] or paper[\"pmcid\"] or \"unknown\"\n",
    "    filename = filename.replace(\"/\", \"_\") + \".pdf\"\n",
    "    print(f\"\\n=== Processing: DOI={paper['doi']}, PMID={paper['pmid']}, PMCID={paper['pmcid']} ===\")\n",
    "    for finder in finders:\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            print(f\"  Trying finder: {finder.__name__} (attempt {attempt}/{max_retries})\")\n",
    "            pdf_url = finder(paper)\n",
    "            if pdf_url:\n",
    "                print(f\"    Found PDF URL: {pdf_url}\")\n",
    "                if download_pdf(pdf_url, out_dir, filename):\n",
    "                    print(f\"    SUCCESS: Downloaded {filename} using {finder.__name__}\")\n",
    "                    return True\n",
    "                else:\n",
    "                    print(f\"    ERROR: Failed to download from {pdf_url}\")\n",
    "            else:\n",
    "                print(f\"    No PDF URL found by {finder.__name__}\")\n",
    "            time.sleep(1)\n",
    "    with open(error_file, \"a\") as ef:\n",
    "        ef.write(f\"{paper['doi'] or 'none'}\\t{paper['pmid'] or 'none'}\\t{paper['pmcid'] or 'none'}\\n\")\n",
    "    print(f\"  FAILED: No PDF found for {filename} after trying all finders.\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def futureMedicine(req,soup,headers):\n",
    "    possibleLinks=soup.find_all('a',attrs={'href':re.compile(\"/doi/pdf\")})\n",
    "    if len(possibleLinks)>0:\n",
    "        print (\"** fetching reprint using the 'future medicine' finder...\")\n",
    "        pdfUrl=getMainUrl(req.url)+possibleLinks[0].get('href')\n",
    "        return pdfUrl\n",
    "    return None\n",
    "\n",
    "def finder_futureMedicine(paper):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    return html_finder_wrapper(paper, futureMedicine, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nejm(req,soup,headers):\n",
    "    possibleLinks=[x for x in soup.find_all('a') if type(x.get('data-download-type'))==str and (x.get('data-download-type').lower()=='article pdf')]\n",
    "        \n",
    "    if len(possibleLinks)>0:\n",
    "        print (\"** fetching reprint using the 'NEJM' finder...\")\n",
    "        pdfUrl=getMainUrl(req.url)+possibleLinks[0].get('href')\n",
    "        return pdfUrl\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pubmed_central_v1(req,soup,headers):\n",
    "    possibleLinks=soup.find_all('a',re.compile('pdf'))\n",
    "    \n",
    "    possibleLinks=[x for x in possibleLinks if 'epdf' not in x.get('title').lower()] #this allows the pubmed_central finder to also work for wiley\n",
    "    \n",
    "    if len(possibleLinks)>0:\n",
    "        print (\"** fetching reprint using the 'pubmed central' finder...\")\n",
    "        pdfUrl=getMainUrl(req.url)+possibleLinks[0].get('href')\n",
    "        return pdfUrl\n",
    "    \n",
    "    return None\n",
    "\n",
    "def finder_pub1(paper):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    return html_finder_wrapper(paper, pubmed_central_v1, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pubmed_central_v2(req,soup,headers):\n",
    "    possibleLinks=soup.find_all('a',attrs={'href':re.compile('/pmc/articles')})\n",
    "        \n",
    "    if len(possibleLinks)>0:\n",
    "        print (\"** fetching reprint using the 'pubmed central' finder...\")\n",
    "        pdfUrl=\"https://www.ncbi.nlm.nih.gov/{}\".format(possibleLinks[0].get('href'))\n",
    "        return pdfUrl\n",
    "    \n",
    "    return None\n",
    "\n",
    "def finder_pub2(paper):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    return html_finder_wrapper(paper, pubmed_central_v2, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def science_direct(req,soup,headers):\n",
    "    newUri=urllib.parse.unquote(soup.find_all('input')[0].get('value'))\n",
    "    req=requests.get(newUri,allow_redirects=True,headers=headers)\n",
    "    soup=BeautifulSoup(req.content,'html.parser')\n",
    "    \n",
    "\n",
    "    possibleLinks=soup.find_all('meta',attrs={'name':'citation_pdf_url'})\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(possibleLinks)>0:\n",
    "        print (\"** fetching reprint using the 'science_direct' finder...\")\n",
    "        req=requests.get(possibleLinks[0].get('content'),headers=headers)\n",
    "        soup=BeautifulSoup(req.content,'html.parser')\n",
    "        \n",
    "        pdfUrl=soup.find_all('a')[0].get('href')\n",
    "        return pdfUrl\n",
    "    return None\n",
    "\n",
    "def finder_scienceDirect(paper):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    return html_finder_wrapper(paper, science_direct, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finders=[\n",
    "#          'genericCitationLabelled',\n",
    "#          'pubmed_central_v2',\n",
    "#          'acsPublications',\n",
    "#          'uchicagoPress',\n",
    "#          'nejm',\n",
    "#          'futureMedicine',\n",
    "#          'science_direct',\n",
    "#          'direct_pdf_link',\n",
    "# ]\n",
    "finders = [\n",
    "        finder_europepmc,\n",
    "        finder_unpaywall,\n",
    "        finder_core,\n",
    "        finder_openaire,\n",
    "        finder_arxiv,\n",
    "        finder_springer,\n",
    "        finder_acsPublications,\n",
    "        finder_genericCitationLabelled,\n",
    "        finder_pub1,\n",
    "        finder_pub2,\n",
    "        finder_scienceDirect\n",
    "        # Add more finders here...\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = []\n",
    "with open(args['pmf'], \"r\") as f:\n",
    "    for line in f:\n",
    "        doi, pmid, pmcid = [x.strip() if x.strip() != \"none\" else None for x in line.strip().split(\",\")]\n",
    "        papers.append({\"doi\": doi, \"pmid\": pmid, \"pmcid\": pmcid})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = {\n",
    "#     'pmf': 'open_access_pmids.txt',\n",
    "#     'pmids': '%#$',\n",
    "#     'out':'fetched_pdfs',\n",
    "#     'errors': 'unfetched_pmids.tsv',\n",
    "#     'maxRetries': 3\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in papers:\n",
    "        fetch_paper_pdf(paper, args['out'], finders, args['errors'], args['maxRetries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
