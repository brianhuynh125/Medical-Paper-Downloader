{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f329f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "from xml.etree import ElementTree as ET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faf1b5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching PMIDs from: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=(\"eczema\" OR \"atopic dermatitis\" OR \"skin inflammation\" OR \"eczema treatment\" OR \"healthcare for eczema patients\" OR \"mental health and eczema\" OR \"eczema and mental health\" OR \"eczema and depression\" OR \"eczema and anxiety\" AND pubmed pmc open access[filter])&mindate=2020-01-01&maxdate=2024-12-31&datetype=pdat&retmode=json&retmax=10000&api_key=5e6b250a2385792b115d07ec55db5ec44908\n",
      "Fetching PubMed summaries in 36 batches (concurrent)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PubMed batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:14<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching from Europe PMC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EuropePMC batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:11<00:00,  7.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching from CrossRef...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CrossRef batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [04:28<00:00, 26.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifiers saved to open_access_pmids.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# === CONFIG ===\n",
    "API_KEY = \"5e6b250a2385792b115d07ec55db5ec44908\"  # Replace with your actual key\n",
    "\n",
    "RATE_LIMIT_DELAY = 0.12\n",
    "MAX_RESULTS = 10000\n",
    "PMID_FILE = \"open_access_pmids.txt\"\n",
    "PUBMED_BATCH_SIZE = 200\n",
    "\n",
    "MAX_WORKERS = 8  # Number of concurrent threads\n",
    "\n",
    "def fetch_pubmed_summaries_batch(batch_pmids):\n",
    "    id_str = \",\".join(batch_pmids)\n",
    "    summary_url = (\n",
    "        \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
    "        \"?db=pubmed\"\n",
    "        f\"&id={id_str}\"\n",
    "        \"&retmode=json\"\n",
    "    )\n",
    "    if API_KEY:\n",
    "        summary_url += f\"&api_key={API_KEY}\"\n",
    "    summary_resp = requests.get(summary_url)\n",
    "    summary_resp.raise_for_status()\n",
    "    summaries = summary_resp.json()[\"result\"]\n",
    "    results = []\n",
    "    for pmid in batch_pmids:\n",
    "        item = summaries.get(pmid, {})\n",
    "        doi = item.get(\"elocationid\") or item.get(\"doi\")\n",
    "        pmcid = item.get(\"pmcid\") if \"pmcid\" in item else None\n",
    "        results.append({\n",
    "            \"doi\": doi,\n",
    "            \"pmid\": pmid,\n",
    "            \"pmcid\": pmcid,\n",
    "            \"source\": \"PubMed\"\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def fetch_pubmed_identifiers(keywords, start_date, end_date, max_results=MAX_RESULTS):\n",
    "    query = \" OR \".join([f'\"{kw}\"' for kw in keywords]) + \" AND pubmed pmc open access[filter]\"\n",
    "    url = (\n",
    "        \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "        \"?db=pubmed\"\n",
    "        f\"&term=({query})\"\n",
    "        f\"&mindate={start_date}&maxdate={end_date}&datetype=pdat\"\n",
    "        f\"&retmode=json&retmax={max_results}\"\n",
    "    )\n",
    "    if API_KEY:\n",
    "        url += f\"&api_key={API_KEY}\"\n",
    "    print(\"Fetching PMIDs from:\", url)\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    pmids = response.json()[\"esearchresult\"][\"idlist\"]\n",
    "\n",
    "    # Concurrent batch fetch summaries\n",
    "    results = []\n",
    "    batches = [pmids[i:i+PUBMED_BATCH_SIZE] for i in range(0, len(pmids), PUBMED_BATCH_SIZE)]\n",
    "    print(f\"Fetching PubMed summaries in {len(batches)} batches (concurrent)...\")\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_to_batch = {executor.submit(fetch_pubmed_summaries_batch, batch): batch for batch in batches}\n",
    "        for future in tqdm(as_completed(future_to_batch), total=len(batches), desc=\"PubMed batches\"):\n",
    "            try:\n",
    "                batch_results = future.result()\n",
    "                results.extend(batch_results)\n",
    "            except Exception as e:\n",
    "                print(f\"Batch failed: {e}\")\n",
    "    return results\n",
    "\n",
    "def fetch_europepmc_identifiers(keywords, start_date, end_date, max_results=MAX_RESULTS):\n",
    "    query = \" OR \".join([f'\"{kw}\"' for kw in keywords])\n",
    "    page_size = 1000\n",
    "    results = []\n",
    "    print(\"Fetching from Europe PMC...\")\n",
    "    for start in tqdm(range(1, max_results+1, page_size), desc=\"EuropePMC batches\"):\n",
    "        url = (\n",
    "            \"https://www.ebi.ac.uk/europepmc/webservices/rest/search\"\n",
    "            f\"?query=({query})+AND+OPEN_ACCESS:Y\"\n",
    "            f\"&resultType=core&format=json&pageSize={page_size}&cursorMark=*&sort_date:y\"\n",
    "        )\n",
    "        r = requests.get(url)\n",
    "        r.raise_for_status()\n",
    "        batch = r.json().get(\"resultList\", {}).get(\"result\", [])\n",
    "        for result in batch:\n",
    "            results.append({\n",
    "                \"doi\": result.get(\"doi\"),\n",
    "                \"pmid\": result.get(\"pmid\"),\n",
    "                \"pmcid\": result.get(\"pmcid\"),\n",
    "                \"source\": \"EuropePMC\"\n",
    "            })\n",
    "        if len(batch) < page_size:\n",
    "            break\n",
    "        time.sleep(0.1)\n",
    "    return results\n",
    "\n",
    "def fetch_crossref_identifiers(keywords, start_date, end_date, max_results=MAX_RESULTS):\n",
    "    query = \" OR \".join(keywords)\n",
    "    rows = 1000\n",
    "    results = []\n",
    "    print(\"Fetching from CrossRef...\")\n",
    "    for offset in tqdm(range(0, max_results, rows), desc=\"CrossRef batches\"):\n",
    "        url = (\n",
    "            \"https://api.crossref.org/works\"\n",
    "            f\"?query={query}\"\n",
    "            f\"&filter=from-pub-date:{start_date},until-pub-date:{end_date},type:journal-article\"\n",
    "            f\"&rows={rows}&offset={offset}\"\n",
    "        )\n",
    "        r = requests.get(url)\n",
    "        r.raise_for_status()\n",
    "        batch = r.json().get(\"message\", {}).get(\"items\", [])\n",
    "        for item in batch:\n",
    "            results.append({\n",
    "                \"doi\": item.get(\"DOI\"),\n",
    "                \"pmid\": None,\n",
    "                \"pmcid\": None,\n",
    "                \"source\": \"CrossRef\"\n",
    "            })\n",
    "        if len(batch) < rows:\n",
    "            break\n",
    "        time.sleep(0.1)\n",
    "    return results\n",
    "\n",
    "def merge_and_rank_identifiers(*sources):\n",
    "    merged = {}\n",
    "    for source in sources:\n",
    "        for item in source:\n",
    "            key = item.get(\"doi\") or item.get(\"pmid\") or item.get(\"pmcid\")\n",
    "            if not key:\n",
    "                continue\n",
    "            if key not in merged:\n",
    "                merged[key] = item\n",
    "            else:\n",
    "                for k in [\"doi\", \"pmid\", \"pmcid\"]:\n",
    "                    if not merged[key].get(k) and item.get(k):\n",
    "                        merged[key][k] = item[k]\n",
    "    ranked = sorted(\n",
    "        merged.values(),\n",
    "        key=lambda x: (x.get(\"doi\") is not None, x.get(\"pmid\") is not None, x.get(\"pmcid\") is not None),\n",
    "        reverse=True\n",
    "    )\n",
    "    return ranked\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = [\n",
    "        \"eczema\", \"atopic dermatitis\", \"skin inflammation\", \"eczema treatment\",\n",
    "        \"healthcare for eczema patients\", \"mental health and eczema\",\n",
    "        \"eczema and mental health\", \"eczema and depression\", \"eczema and anxiety\"\n",
    "    ]\n",
    "    start_date = \"2020-01-01\"\n",
    "    end_date = \"2024-12-31\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    pubmed_results = fetch_pubmed_identifiers(keywords, start_date, end_date)\n",
    "    europepmc_results = fetch_europepmc_identifiers(keywords, start_date, end_date)\n",
    "    crossref_results = fetch_crossref_identifiers(keywords, start_date, end_date)\n",
    "    all_results = merge_and_rank_identifiers(pubmed_results, europepmc_results, crossref_results)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    with open(PMID_FILE, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"doi\", \"pmid\", \"pmcid\"])\n",
    "        for item in all_results:\n",
    "            writer.writerow([\n",
    "                item.get(\"doi\") or \"none\",\n",
    "                item.get(\"pmid\") or \"none\",\n",
    "                item.get(\"pmcid\") or \"none\"\n",
    "            ])\n",
    "    print(f\"Identifiers saved to {PMID_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a309f785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Scanning 9951 XML files for open-access PDFs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting PMC IDs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9951/9951 [00:03<00:00, 2990.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Limiting download to first 10 PMC IDs.\n",
      "âœ… Ready to download 10 PDFs concurrently with 8 workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading PDFs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "Downloaded PDFs: 0\n",
      "Skipped (no PDF found): 10\n",
      "Skipped PMC IDs saved to skipped_pmc_ids.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from xml.etree import ElementTree as ET\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "PDF_SAVE_FOLDER = \"pubmed_pdfs\"\n",
    "PDF_RATE_LIMIT_DELAY = 0.3  # seconds between requests\n",
    "PDF_RETRY_LIMIT = 5\n",
    "MAX_WORKERS = 8  # Adjust for your bandwidth, 5-10 is usually safe\n",
    "\n",
    "def extract_pmc_id_from_xml(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        for article_id in root.iter(\"ArticleId\"):\n",
    "            if article_id.attrib.get(\"IdType\") == \"pmc\":\n",
    "                return article_id.text.replace(\"PMC\", \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"â— Error parsing {xml_path}: {e}\")\n",
    "    return None\n",
    "\n",
    "def download_pmc_pdf(pmc_id, folder_path):\n",
    "    pdf_url = f\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{pmc_id}/pdf/\"\n",
    "    for attempt in range(PDF_RETRY_LIMIT):\n",
    "        try:\n",
    "            response = requests.get(pdf_url, allow_redirects=True, timeout=30)\n",
    "            if response.status_code == 200 and 'application/pdf' in response.headers.get('Content-Type', ''):\n",
    "                os.makedirs(folder_path, exist_ok=True)\n",
    "                file_path = os.path.join(folder_path, f\"PMC{pmc_id}.pdf\")\n",
    "                with open(file_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                return (pmc_id, True)\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"â— Rate limited by NCBI. Waiting before retrying PMC{pmc_id}...\")\n",
    "                time.sleep(10 + random.uniform(0, 5))\n",
    "            else:\n",
    "                return (pmc_id, False)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"â— PDF download error for PMC{pmc_id} (attempt {attempt+1}): {e}\")\n",
    "            time.sleep(2 ** attempt + random.uniform(0, 1))\n",
    "    return (pmc_id, False)\n",
    "\n",
    "def batch_download_open_access_pdfs(xml_folder, pdf_folder, limit=None, max_workers=MAX_WORKERS):\n",
    "    xml_files = [os.path.join(xml_folder, f) for f in os.listdir(xml_folder) if f.endswith(\".xml\")]\n",
    "    print(f\"ðŸ”Ž Scanning {len(xml_files)} XML files for open-access PDFs...\")\n",
    "    pmc_ids = []\n",
    "    for xml_path in tqdm(xml_files, desc=\"Extracting PMC IDs\"):\n",
    "        pmc_id = extract_pmc_id_from_xml(xml_path)\n",
    "        if pmc_id:\n",
    "            pmc_ids.append(pmc_id)\n",
    "\n",
    "    if limit is not None:\n",
    "        pmc_ids = pmc_ids[:limit]\n",
    "        print(f\"âš¡ Limiting download to first {limit} PMC IDs.\")\n",
    "\n",
    "    print(f\"âœ… Ready to download {len(pmc_ids)} PDFs concurrently with {max_workers} workers.\")\n",
    "    skipped = []\n",
    "    downloaded = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_pmc = {executor.submit(download_pmc_pdf, pmc_id, pdf_folder): pmc_id for pmc_id in pmc_ids}\n",
    "        for future in tqdm(as_completed(future_to_pmc), total=len(future_to_pmc), desc=\"Downloading PDFs\"):\n",
    "            pmc_id, success = future.result()\n",
    "            if success:\n",
    "                downloaded += 1\n",
    "            else:\n",
    "                skipped.append(pmc_id)\n",
    "            time.sleep(PDF_RATE_LIMIT_DELAY)\n",
    "\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"Downloaded PDFs: {downloaded}\")\n",
    "    print(f\"Skipped (no PDF found): {len(skipped)}\")\n",
    "    if skipped:\n",
    "        with open(\"skipped_pmc_ids.txt\", \"w\") as f:\n",
    "            for pmc_id in skipped:\n",
    "                f.write(f\"{pmc_id}\\n\")\n",
    "        print(f\"Skipped PMC IDs saved to skipped_pmc_ids.txt\")\n",
    "\n",
    "# === USAGE ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Only process files already in pubmed_articles\n",
    "    # Set limit=50 for a test run, or remove for all\n",
    "    batch_download_open_access_pdfs(\"pubmed_articles\", \"pubmed_pdfs\", limit=10, max_workers=8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
